{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"Elementwise equality test\n",
    "\n",
    "A simple sameness test will be deployed on 2 arbitrary files. The test will compare \n",
    "some arbitrary data columns producing flag columns that indicate that there is element \n",
    "level equivalence between the 2 data columns. The test will flag if the 2 are \n",
    "equivalent with the following key requirements:\n",
    "\n",
    "  1. flag only if the elements are identical for a run of more than some threshold, t\n",
    "  2. flag identical only if elements are equal after truncating the more precise \n",
    "     number to the number of significant digits of the least precise.\n",
    "  3. NaN values will be ignored, i.e. marked as equal if either series \n",
    "     contains them. this means that runs will not be broken for NaN values\n",
    "\n",
    "Examples:\n",
    "  * \"ALIJAR we detected turbine A1, A3, A7 have identical realpower readings \n",
    "    between yyyy-mm-dd hh:mm:ss and yyyy-mm-dd hh:mm:ss\"\n",
    "        \n",
    "Attributes:\n",
    "    \n",
    "    \n",
    "To do:\n",
    "    * Test with different date ranges\n",
    "    * Integrate with testing framework\n",
    "    * How does this interact with other tests?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('/Users/sentient-asoellinger/code/ingestigator-processing/lambda/scada/tests/multi-file/src')\n",
    "import s3ss_python2 as s3ss\n",
    "\n",
    "def _rolling_count(val):\n",
    "    \n",
    "    if val == _rolling_count.previous:\n",
    "        \n",
    "        _rolling_count.count +=1\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        _rolling_count.previous = val\n",
    "        \n",
    "        _rolling_count.count = 1\n",
    "        \n",
    "    return _rolling_count.count\n",
    "\n",
    "def _rolling_index(val):\n",
    "    \n",
    "    if val != _rolling_index.previous:\n",
    "        \n",
    "        _rolling_index.previous = val\n",
    "        \n",
    "        _rolling_index.count += 1\n",
    "        \n",
    "    return _rolling_index.count\n",
    "\n",
    "def _find_sig_figs(number):\n",
    "    \n",
    "    return len(str(number).split('.')[-1])\n",
    "\n",
    "def _truncate(number, n):\n",
    "    '''\n",
    "    Truncates/pads a float f to n \n",
    "    decimal places without rounding\n",
    "    '''\n",
    "    \n",
    "    int_n = int(n)\n",
    "    \n",
    "    s = '{}'.format(number)\n",
    "    \n",
    "    if 'e' in s or 'E' in s:\n",
    "        \n",
    "        return '{0:.{1}f}'.format(number, int_n)\n",
    "    \n",
    "    i, p, d = s.partition('.')\n",
    "    \n",
    "    return '.'.join([i, (d + '0' * int_n)[:int_n]])\n",
    "\n",
    "def _split_mergeddf_header(header):\n",
    "    \n",
    "    temp = header.split('-')\n",
    "    \n",
    "    return {\n",
    "        'scada_plant': temp[0],\n",
    "        'scada_device': temp[1],\n",
    "        'metric': temp[2],\n",
    "        'statistic': temp[3],\n",
    "        'header': header\n",
    "    }\n",
    "\n",
    "def _condense_names(name1, name2):\n",
    "    \n",
    "    a = set([name1, name2])\n",
    "    \n",
    "    return '-'.join(list(a))\n",
    "\n",
    "def _make_element_equality_base_header(metricB):\n",
    "    \n",
    "    #wtg = _condense_names(metricA['scada_plant'] + '+' + metricA['scada_device'],\n",
    "    #                      metricB['scada_plant'] + '+' + metricB['scada_device'])\n",
    "    #metric = _condense_names(metricA['metric'], metricB['metric'])\n",
    "    #stat = _condense_names(metricA['statistic'], metricB['statistic'])\n",
    "    \n",
    "    return metricB['scada_plant'] + '+' + metricB['scada_device'] + \\\n",
    "           '_' + metricB['metric'] + '_' + metricB['statistic']\n",
    "\n",
    "def _compare_equality(elem1, elem2):\n",
    "    \n",
    "    if pd.isnull(elem1) or pd.isnull(elem2):\n",
    "        \n",
    "        return 1\n",
    "    \n",
    "    if elem1 == elem2:\n",
    "        \n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def compare_element_equality(merged_df, min_periods):\n",
    "    \n",
    "    fields = [col for col in merged_df.columns if col != 'ts']\n",
    "    \n",
    "    combos = set()\n",
    "    \n",
    "    for i, fi in enumerate(fields):\n",
    "        \n",
    "        for j, fj in enumerate(fields):\n",
    "            \n",
    "            if i == j: continue\n",
    "                \n",
    "            combos.add(tuple(sorted([fi, fj])))\n",
    "    \n",
    "    mask = []\n",
    "\n",
    "    for combo in combos:\n",
    "        \n",
    "        metricA = _split_mergeddf_header(combo[0])\n",
    "        metricB = _split_mergeddf_header(combo[1])\n",
    "        \n",
    "        new_header = _make_element_equality_base_header(metricB)\n",
    "        \n",
    "        # use truncated variables\n",
    "        merged_df[combo[0] + '_sig_figs'] = merged_df[combo[0]].apply(_find_sig_figs)\n",
    "        merged_df[combo[1] + '_sig_figs'] = merged_df[combo[1]].apply(_find_sig_figs)\n",
    "        \n",
    "        #merged_df[[combo[0], combo[0] + '_sig_figs']] \\\n",
    "        #    .apply(lambda x: _print(x[combo[0]], x[combo[0] + '_sig_figs']))\n",
    "        merged_df['min_sig_figs'] = merged_df[[combo[0] + '_sig_figs',\n",
    "                                               combo[1] + '_sig_figs']] \\\n",
    "                                             .apply(lambda x: min(x[0], x[1]), axis=1)\n",
    "        \n",
    "        merged_df[combo[0] + '_trunc'] = merged_df[[combo[0], 'min_sig_figs']] \\\n",
    "                                         .apply(lambda x: _truncate(x[0], x[1]), axis=1)\n",
    "            \n",
    "        merged_df[combo[1] + '_trunc'] = merged_df[[combo[1], 'min_sig_figs']] \\\n",
    "                                         .apply(lambda x: _truncate(x[0], x[1]), axis=1)\n",
    "        \n",
    "        # check equality\n",
    "        analysis_header = new_header + '_equality'\n",
    "        merged_df[analysis_header] = \\\n",
    "            merged_df[[combo[0] + '_trunc', combo[1] + '_trunc']] \\\n",
    "            .apply(lambda elems: _compare_equality(elems[0], elems[1]), axis=1)\n",
    "        \n",
    "        # take account of the count the number of running flags\n",
    "        account_header = new_header + '_rolling_count'\n",
    "        _rolling_count.count = 0\n",
    "        _rolling_count.previous = None\n",
    "        merged_df[account_header] = merged_df[analysis_header].apply(_rolling_count)\n",
    "        \n",
    "        # give each run a unique id\n",
    "        _rolling_index.count = 0\n",
    "        _rolling_index.previous = None\n",
    "        srs_index = new_header + '_series_index'\n",
    "        merged_df[srs_index] = merged_df[analysis_header].apply(_rolling_index)\n",
    "        \n",
    "        # check for runs that lasted longer than min_periods\n",
    "        good_periods = merged_df[(merged_df[account_header] >= min_periods) \n",
    "                                & merged_df[analysis_header] == 1][srs_index] \\\n",
    "                       .drop_duplicates().as_matrix()\n",
    "        \n",
    "        # flag rows that are part of a run longer than min_periods\n",
    "        flag_header = new_header + '_elementEquality(' + str(min_periods) + ')'\n",
    "        merged_df[flag_header] = merged_df[srs_index].isin(good_periods)\n",
    "    \n",
    "    return merged_df[flag_header]\n",
    "    #return merged_df\n",
    "\n",
    "def parse_key(key):\n",
    "    \n",
    "    temp  = key.split('/')\n",
    "    comps = ['data_type', 'step', 'scada_plant', \n",
    "             'scada_device', 'metric', 'fn']\n",
    "    \n",
    "    out = dict(zip(comps, temp))\n",
    "    \n",
    "    pth = out['fn'].split('.')[:-2]\n",
    "    comps = ['scada_plant', 'scada_device', \n",
    "             'metric', 'start', 'end']\n",
    "    \n",
    "    out = dict([tuple([k, v]) for k, v in out.iteritems()] + zip(comps, pth))\n",
    "    out['key'] = key\n",
    "    \n",
    "    return out\n",
    "\n",
    "def pkey_to_column_header(pkey, stat):\n",
    "    \n",
    "    return pkey['scada_plant'] + '-' \\\n",
    "           + pkey['scada_device'] + '-' \\\n",
    "           + pkey['metric'] + '-' \\\n",
    "           + stat\n",
    "\n",
    "def get_and_combine_dfs(bucket, df1, pkey1, pkey2, temp_dir, on, stat):\n",
    "        \n",
    "    if not os.path.exists(temp_dir+pkey2['fn']):\n",
    "\n",
    "        s3ss.get_save_object(bucket=bucket, key=pkey2['key'], \n",
    "                             aws_cred='soellingeraj', outfile=temp_dir+pkey2['fn'])\n",
    "    \n",
    "    df2 = pd.read_csv(temp_dir+pkey2['fn'], usecols=[on, stat], compression='bz2')\n",
    "    \n",
    "    # combine dataframes\n",
    "    merged_df = df1[['ts', stat]].merge(df2, on=on, how='outer')\n",
    "    merged_df.index = merged_df[on]\n",
    "    merged_df = merged_df.drop([on], axis=1)\n",
    "    \n",
    "     # set field name convention\n",
    "    merged_df.columns = [pkey_to_column_header(pkey1, stat), \n",
    "                         pkey_to_column_header(pkey2, stat)]\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def main(merged_df, min_rows):\n",
    "    \n",
    "    return compare_element_equality(merged_df, min_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and cleaned up.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    _bucket = 'sentient-science-customer-acciona'\n",
    "\n",
    "    _out_key = 'scada/tested/'\n",
    "    \n",
    "    _key_base = 'scada/mapped/'\n",
    "\n",
    "    _scada_plant = 'ADRANO'\n",
    "\n",
    "    _key_base = key_base + scada_plant\n",
    "\n",
    "    _temp_dir = '/Users/sentient-asoellinger/Downloads/temp/'\n",
    "\n",
    "    _min_hrs = 3\n",
    "\n",
    "    _min_rows = 3 * 6\n",
    "\n",
    "    _tgt_metric = 'realpower'\n",
    "\n",
    "    _tgt_stat = 'avg'\n",
    "\n",
    "    _files = list(s3ss.get_matching_s3_objects(bucket, 'soellingeraj', \n",
    "                                               prefix=key_base))\n",
    "\n",
    "    _rolling_count.count = 0\n",
    "    _rolling_count.previous = None\n",
    "\n",
    "    _rolling_index.count = 0\n",
    "    _rolling_index.previous = None\n",
    "\n",
    "    _inventory = {}\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        pkey = parse_key(file['Key'])\n",
    "\n",
    "        if pkey['metric'] not in _inventory: _inventory[pkey['metric']] = []\n",
    "\n",
    "        _inventory[pkey['metric']].append(pkey)\n",
    "\n",
    "    _tested = _inventory[_tgt_metric][0]\n",
    "\n",
    "    s3ss.get_save_object(bucket=bucket, key=_tested['key'], \n",
    "                                 aws_cred='soellingeraj', outfile=temp_dir+_tested['fn'])\n",
    "\n",
    "    _df1 = pd.read_csv(temp_dir+_tested['fn'], compression='bz2')\n",
    "\n",
    "    _df1.index = _df1['ts']\n",
    "\n",
    "    for pkey in _inventory[_tgt_metric]:\n",
    "\n",
    "        # don't compare _tested to itself\n",
    "        if pkey == _tested: continue\n",
    "\n",
    "        # make sure that the time ranges of the files overlap\n",
    "        if float(_tested['end']) < float(pkey['start']): continue\n",
    "        if float(_tested['start']) > float(pkey['end']): continue\n",
    "\n",
    "        # download data to temp and load dataframes\n",
    "        _merged_df = get_and_combine_dfs(_bucket, _df1, _tested, pkey, \n",
    "                                         _temp_dir, 'ts', _tgt_stat)\n",
    "\n",
    "        # get analysis\n",
    "        new_df = main(_merged_df, _min_rows)\n",
    "\n",
    "        # update _df1 with new flag column\n",
    "        _df1 = _df1.merge(new_df.to_frame(), left_index=True, right_index=True)\n",
    "\n",
    "        # delete df2 from temp\n",
    "        os.remove(_temp_dir+pkey['fn'])\n",
    "\n",
    "    os.remove(_temp_dir+_tested['fn'])\n",
    "    _df1 = _df1.drop(['ts'], axis=1)\n",
    "    print 'Finished and cleaned up.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
